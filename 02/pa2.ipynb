{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './IRTM'\n",
    "OPT_DIR = './OPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/wayne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = []\n",
    "\n",
    "full_filenames = os.listdir (DATA_DIR)\n",
    "for full_filename in full_filenames:\n",
    "    filename, extname = full_filename.split('.')\n",
    "    if extname == 'txt':\n",
    "        filename_list.append(int(filename))\n",
    "\n",
    "filename_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = str.maketrans('', '', string.punctuation.replace(\"-\", \"\"))\n",
    "\n",
    "def tokenize_data(content):\n",
    "    # Tokenize content\n",
    "    tokens = content.translate(punct).replace('\\n', '').split(' ')\n",
    "    # Lower case\n",
    "    lower_tokens = list(map(lambda word: word.lower(), tokens))\n",
    "    # Stemming using Porter's algorithm\n",
    "    porter = PorterStemmer()\n",
    "    stemed_tokens = list(map(lambda word: porter.stem(word), lower_tokens))\n",
    "    # Stopword removal\n",
    "    filtered_tokens = [word for word in stemed_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency = Counter()\n",
    "document_token_raw = []\n",
    "document_token = []\n",
    "document_tfs = []\n",
    "\n",
    "for filename in filename_list:\n",
    "    file = open('{}/{}.txt'.format(DATA_DIR, filename), 'r')\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "\n",
    "    tokens = tokenize_data(content)\n",
    "    document_token_raw.append(tokens)\n",
    "    filtered_tokens = filter(lambda x: x != '', tokens)\n",
    "    filtered_tokens = filter(lambda x: x[0] not in string.punctuation, filtered_tokens)\n",
    "    filtered_tokens = list(filter(lambda x: not str.isdigit(x[0][0]), filtered_tokens))\n",
    "\n",
    "    word_frequencies = Counter(filtered_tokens)\n",
    "    document_token.append(list(word_frequencies.keys()))\n",
    "    tf = {}\n",
    "    for token in filtered_tokens:\n",
    "        if tf.get(token):\n",
    "            tf[token] += 1\n",
    "        else:\n",
    "            tf[token] = 1\n",
    "    document_tfs.append(tf)\n",
    "    \n",
    "    document_frequency.update(word_frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = sorted(document_frequency.most_common(), key=lambda df: df[0])\n",
    "dictionary = [(index + 1, item[0], item[1]) for index, item in enumerate(dictionary)]\n",
    "dictionary_dict = { item[1]: item for item in dictionary }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dictionary.txt', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\"\\t\")\n",
    "    writer.writerow(['t_index', 'term', 'df'])\n",
    "    \n",
    "    for item in dictionary:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(filename_list)\n",
    "document_tfidf = []\n",
    "\n",
    "for index, tokens in enumerate(document_token):\n",
    "    document_tf = document_tfs[index]\n",
    "    total_freq = sum(document_tf.values())\n",
    "    tfs = list(map(lambda item: (item[0], item[1] / total_freq), document_tf.items() ))\n",
    "    dfs = [dictionary_dict.get(token) for token in tokens]\n",
    "    idfs = [(df[0], math.log10(n / df[2])) for df in dfs]\n",
    "    tfidfs = [(idfs[index][0], tf[1] * idfs[index][1]) for index, tf in enumerate(tfs)]\n",
    "    sorted_tfidfs = sorted(tfidfs, key=lambda tfidf: tfidf[0])\n",
    "    document_tfidf.append(tfidfs)\n",
    "    \n",
    "    with open('{}/{}.txt'.format(OPT_DIR, index + 1), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\"\\t\")\n",
    "        writer.writerow(['t_index', 'tf-idf'])\n",
    "        \n",
    "        for tfidf in sorted_tfidfs:\n",
    "            writer.writerow(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    sum_xx, sum_xy, sum_yy = 0.0, 0.0, 0.0\n",
    "    for i in range(0, len(v1)):\n",
    "        x, y = v1[i], v2[i]\n",
    "        sum_xx += math.pow(x, 2)\n",
    "        sum_yy += math.pow(y, 2)\n",
    "        sum_xy += x * y\n",
    "    try:\n",
    "        return sum_xy / math.sqrt(sum_xx * sum_yy)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(tokens, tfidfs):\n",
    "    vector = []\n",
    "    for token in tokens:\n",
    "        dictionary_item = dictionary_dict.get(token)\n",
    "        if dictionary_item is None:\n",
    "            vector.append(0)\n",
    "        else:\n",
    "            dictionary_index = dictionary_item[0]\n",
    "            tfidf = next(filter(lambda tfidf: tfidf[0] == dictionary_index, tfidfs), None)\n",
    "            vector.append(tfidf[1])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5237166991853831\n"
     ]
    }
   ],
   "source": [
    "vec_doc1 = get_document_vector(document_token_raw[0], document_tfidf[0])\n",
    "vec_doc2 = get_document_vector(document_token_raw[1], document_tfidf[1])\n",
    "\n",
    "print(cosine_similarity(vec_doc1, vec_doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13253, 0.00480410381193267),\n",
       " (5509, 0.0033679205813060453),\n",
       " (409, 0.0021229161184471955),\n",
       " (6352, 0.0054243695501198395),\n",
       " (2159, 0.003753021049379959),\n",
       " (13124, 0.0061398760269328655),\n",
       " (13524, 0.00546367299339346),\n",
       " (8343, 0.022644152204653555),\n",
       " (4421, 0.005613191231996028),\n",
       " (11389, 0.0053475314745948885),\n",
       " (9159, 0.013766851758178364),\n",
       " (9153, 0.0017730987814276848),\n",
       " (10982, 0.011047571459512637),\n",
       " (7481, 0.032199152499223394),\n",
       " (13372, 0.01266717310490319),\n",
       " (7904, 0.014429420735685665),\n",
       " (6197, 0.015668752271508244),\n",
       " (7002, 0.02350613451785779),\n",
       " (1096, 0.0305710035051805),\n",
       " (12191, 0.006014004587513863),\n",
       " (10599, 0.012999007858030227),\n",
       " (3892, 0.0073354260569287255),\n",
       " (4731, 0.008153660526182336),\n",
       " (11505, 0.026117533963808105),\n",
       " (12010, 0.0028569386584200874),\n",
       " (12431, 0.013767318322588932),\n",
       " (10051, 0.010640971751752135),\n",
       " (11842, 0.013767318322588932),\n",
       " (2283, 0.007268355203603601),\n",
       " (12183, 0.01678388544066355),\n",
       " (10368, 0.0031336403036691125),\n",
       " (7881, 0.010225788961836211),\n",
       " (11452, 0.013301546287014666),\n",
       " (1078, 0.00506139512872334),\n",
       " (9358, 0.010225788961836211),\n",
       " (7097, 0.00431801617487949),\n",
       " (11437, 0.00965573006178882),\n",
       " (9156, 0.0040999496373760646),\n",
       " (3653, 0.0033246932652934227),\n",
       " (8739, 0.004181964569701471),\n",
       " (3378, 0.004255551610113055),\n",
       " (71, 0.005564661376552365),\n",
       " (12912, 0.006014004587513863),\n",
       " (5491, 0.009276565419277537),\n",
       " (8303, 0.001579777103141135),\n",
       " (8775, 0.005870714317752424),\n",
       " (12123, 0.006368748355341587),\n",
       " (4480, 0.003877663985085477),\n",
       " (13474, 0.002069912693891193),\n",
       " (219, 0.004255551610113055),\n",
       " (5625, 0.005648428665461837),\n",
       " (12051, 0.004978173651892209),\n",
       " (10597, 0.007936576700868373),\n",
       " (7148, 0.007462130044317606),\n",
       " (1690, 0.0028206323184707333),\n",
       " (13187, 0.003942342716270555),\n",
       " (9294, 0.005443945687466473),\n",
       " (6612, 0.001987330200707116),\n",
       " (12058, 0.0064148179430316985),\n",
       " (7647, 0.0036633741872815068),\n",
       " (11679, 0.0061142007010361),\n",
       " (9617, 0.006883425879089182),\n",
       " (9409, 0.004182555974925879),\n",
       " (3348, 0.00629959512130707),\n",
       " (9075, 0.0031188917461472602),\n",
       " (2394, 0.008519703012644929),\n",
       " (9742, 0.014508637002261763),\n",
       " (2281, 0.010225788961836211),\n",
       " (13152, 0.012566612542259962),\n",
       " (4769, 0.0034566334332461583),\n",
       " (9994, 0.012566612542259962),\n",
       " (1806, 0.0047888519850016135),\n",
       " (8952, 0.00457037076144534),\n",
       " (2197, 0.009106190737305085),\n",
       " (7495, 0.008097978161732725),\n",
       " (12879, 0.0035298907373286733),\n",
       " (5260, 0.008330381345754333),\n",
       " (3579, 0.006918183876798126),\n",
       " (8323, 0.0029284404425268013),\n",
       " (5123, 0.005044532536655086),\n",
       " (13375, 0.005236441214827983),\n",
       " (5000, 0.006684259601083491),\n",
       " (3823, 0.006440773217799815),\n",
       " (13172, 0.002730742789696918),\n",
       " (3657, 0.008330381345754333),\n",
       " (9872, 0.010356826577784166),\n",
       " (3755, 0.00788496538141246),\n",
       " (13387, 0.0077847692678902234),\n",
       " (1240, 0.014337377222636321),\n",
       " (7070, 0.0072877455374868155),\n",
       " (4768, 0.010101146026130693),\n",
       " (12315, 0.0031109947586442648),\n",
       " (10581, 0.007736107350443224),\n",
       " (3157, 0.010101146026130693),\n",
       " (11248, 0.007736107350443224),\n",
       " (13370, 0.0063272134813563636),\n",
       " (5829, 0.006218487565509228),\n",
       " (9484, 0.013906842620527868),\n",
       " (11349, 0.0056272279344687474),\n",
       " (872, 0.017878906583389042),\n",
       " (5056, 0.0009565831939731563),\n",
       " (10819, 0.009459119735394999),\n",
       " (10843, 0.008870207071375775),\n",
       " (6119, 0.011326298628642944),\n",
       " (6681, 0.004408114751844818),\n",
       " (6247, 0.00865419055946554),\n",
       " (4591, 0.00845502428145985),\n",
       " (5715, 0.0050111376372922005),\n",
       " (13057, 0.0061398760269328655),\n",
       " (1154, 0.005669806810491802),\n",
       " (5255, 0.006529383490952026),\n",
       " (6820, 0.009366212281375987),\n",
       " (8996, 0.013301546287014666),\n",
       " (8373, 0.005757154581308973),\n",
       " (9515, 0.0055237857297563185),\n",
       " (10812, 0.013301546287014666),\n",
       " (3442, 0.007989252245885587),\n",
       " (977, 0.008097978161732725),\n",
       " (6669, 0.0033076050850380867),\n",
       " (5472, 0.0040539182402469075),\n",
       " (1923, 0.005182656629887573),\n",
       " (2082, 0.007376966377443353),\n",
       " (3267, 0.011136976961752348),\n",
       " (12082, 0.007025388700952237),\n",
       " (6573, 0.007736107350443224),\n",
       " (7249, 0.003595760736160872),\n",
       " (4794, 0.00480410381193267),\n",
       " (7968, 0.002566292626799734),\n",
       " (6769, 0.00540494228217816),\n",
       " (11530, 0.006953421310263934),\n",
       " (8409, 0.007834376135754122),\n",
       " (7979, 0.0029431648257844736)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_token_raw[0]\n",
    "document_tfidf[0]\n",
    "# dictionary_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
