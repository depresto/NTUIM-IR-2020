{
 "cells": [
  {
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from collections import Counter"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(f_obs, f_exp):\n",
    "    if (len(f_obs) != len(f_exp)):\n",
    "        raise Exception('Size of observed frequencies and expected frequencies is not matched')\n",
    "    else:\n",
    "        return reduce(lambda acc, obs: acc + (obs[1] - f_exp[obs[0]])**2 / f_exp[obs[0]], enumerate(f_obs), 0)\n",
    "\n",
    "def tokenize_data(content):\n",
    "    punct = str.maketrans('', '', string.punctuation.replace(\"-\", \"\"))\n",
    "    # Tokenize content\n",
    "    tokens = content.translate(punct).replace('\\n', '').split(' ')\n",
    "    # Lower case\n",
    "    lower_tokens = list(map(lambda word: word.lower(), tokens))\n",
    "    # Stemming using Porter's algorithm\n",
    "    porter = PorterStemmer()\n",
    "    stemed_tokens = list(map(lambda word: porter.stem(word), lower_tokens))\n",
    "    # Stopword removal\n",
    "    filtered_tokens = [\n",
    "        word for word in stemed_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_tokens\n",
    "\n",
    "def read_dataset():\n",
    "    filename_list = []\n",
    "    full_filenames = os.listdir('./IRTM')\n",
    "    for full_filename in full_filenames:\n",
    "        filename, extname = full_filename.split('.')\n",
    "        if extname == 'txt':\n",
    "            filename_list.append(int(filename))\n",
    "\n",
    "    document_token = {}\n",
    "    for filename in filename_list:\n",
    "        file = open('{}/{}.txt'.format('./IRTM', filename), 'r')\n",
    "        content = file.read()\n",
    "        file.close()\n",
    "\n",
    "        tokens = tokenize_data(content)\n",
    "\n",
    "        filtered_tokens = filter(lambda x: x != '', tokens)\n",
    "        filtered_tokens = filter(lambda x: x[0] not in string.punctuation, filtered_tokens)\n",
    "        filtered_tokens = list(filter(lambda x: not str.isdigit(x[0][0]), filtered_tokens))\n",
    "\n",
    "        document_token[filename] = list(set(filtered_tokens))\n",
    "\n",
    "    return document_token\n",
    "\n",
    "def read_training_data(document_token):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    training_class_ids = []\n",
    "    training_doc_ids_dict = {}\n",
    "    with open('./training.txt') as training_file:\n",
    "        training_raw = training_file.read()\n",
    "\n",
    "        for row in training_raw.split('\\n'):\n",
    "            class_id, *doc_ids = row.split(' ')[:-1]\n",
    "            class_id = int(class_id)\n",
    "            for doc_id in doc_ids:\n",
    "                y_train.append(class_id)\n",
    "                x_train.append(document_token[int(doc_id)])\n",
    "            \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_token = read_dataset()\n",
    "x_train, y_train = read_training_data(document_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleNBClassifier:\n",
    "    def __init__(self):\n",
    "        self.condprob = {}\n",
    "        self.prior = {}\n",
    "        self.C = []\n",
    "        self.V = []\n",
    "\n",
    "    def read_model(self):\n",
    "        with open('./model.pickle', 'rb') as file:\n",
    "            self.condprob, self.prior, self.V, self.C = pickle.load(file)\n",
    "\n",
    "    '''\n",
    "    TrainMultinomialNB(C, D)\n",
    "        V <- ExtractVocabulary(D)\n",
    "        N <- CountDocs(D)\n",
    "\n",
    "        for each c in C\n",
    "        do \n",
    "            Nc <- CountDocsInClass(D, c)\n",
    "            prior[c] <- Nc / N\n",
    "            textc <- ConcatenateTextOfAllDocsInClass(D, c)\n",
    "\n",
    "            for each t in V\n",
    "            do \n",
    "                Tct <- CountTokensOfTerm(textc, t)\n",
    "            for each t in V\n",
    "            do \n",
    "                condprob[t][c] <- (Tct+1) / ∑(Tct’+1)\n",
    "\n",
    "        return V, prior, condprob\n",
    "    '''\n",
    "    def fit(self, x, y, k_features = float('nan'), method = 'chi_square', save = False):\n",
    "        self.V = self._extract_vocabulary(x)\n",
    "        N = np.unique(x).shape[0]\n",
    "        self.C = np.unique(y)\n",
    "        C_len = self.C.shape[0]\n",
    "        \n",
    "        for c in self.C:\n",
    "            print('Training class', c)\n",
    "            if k_features != k_features: # is nan\n",
    "                selected_V = self.V\n",
    "            else:\n",
    "                selected_V = self._select_features(x, y, c, int(k_features / C_len), method)\n",
    "\n",
    "            N_c = y.count(c)\n",
    "            self.prior[c] = N_c / N\n",
    "            text_c = self._concatenate_text_of_all_docs_in_class(x, y, c)\n",
    "            \n",
    "            uniq_text_c_len = len(set(text_c))\n",
    "            selected_V_len = len(selected_V)\n",
    "            for t in selected_V:\n",
    "                T_ct = text_c.count(c)\n",
    "                self.condprob[t, c] = (T_ct + 1) / (uniq_text_c_len + selected_V_len)\n",
    "\n",
    "        if save:\n",
    "            with open('./model-{}.pickle'.format(method), 'wb') as file:\n",
    "                pickle.dump([self.condprob, self.prior, self.V, self.C], file)\n",
    "            print('Model saved!')\n",
    "\n",
    "        print('Training done')\n",
    "\n",
    "    '''\n",
    "    ApplyMultinomialNB(C, V, prior, condprob, d)\n",
    "        W <- ExtractTokensFromDoc(V, d)\n",
    "        for each c in C\n",
    "        do \n",
    "            score[c] <- log prior[c]\n",
    "            for each t in W\n",
    "            do \n",
    "                score[c] += log condprob[t][c]\n",
    "\n",
    "        return argmaxcscore[c]\n",
    "    '''\n",
    "    def predict_proba(self, W):\n",
    "        score = {}\n",
    "        for c in self.C:\n",
    "            score[c] = math.log(self.prior[c])\n",
    "            for t in W:\n",
    "                if self.condprob.get((t, c)):\n",
    "                    # print(math.log(self.condprob[t, c]), self.condprob[t, c], t, c)\n",
    "                    score[c] += math.log(self.condprob[t, c])\n",
    "\n",
    "        sorted_score = {k: v for k, v in sorted(score.items(), key=lambda item: item[1])}\n",
    "        return sorted_score\n",
    "    \n",
    "    def predict(self, W):\n",
    "        sorted_score = self.predict_proba(W)\n",
    "        return list(sorted_score.keys())[0]\n",
    "            \n",
    "    def _concatenate_text_of_all_docs_in_class(self, x, y, c):\n",
    "        word_list = []\n",
    "        for index, cls in enumerate(y):\n",
    "            if cls == c:\n",
    "                word_list.extend(x[index])\n",
    "        return word_list\n",
    "    '''\n",
    "    SelectFeatures(D, c, k)\n",
    "      V <- ExtractVocabuliary(D)\n",
    "      L <- []\n",
    "      for each t in V\n",
    "      do\n",
    "          A(t,c) <- ComputeFeatureUtility(D,t,c)\n",
    "          Append(L, <t, A(t,c)>)\n",
    "      return FeaturesWithLargestValues(L,k)\n",
    "    '''\n",
    "    def _select_features(self, x, y, c, k_features, method):\n",
    "        V = self._extract_vocabulary(x)\n",
    "        L = {}\n",
    "        for t in V:\n",
    "            l = self._compute_feature_utility(x, y, t, c, method)\n",
    "            if L.get(t):\n",
    "                if l > L[t]:\n",
    "                    L[t] = l\n",
    "            else:\n",
    "                L[t] = l\n",
    "        sorted_L = {k: v for k, v in sorted(L.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return list(sorted_L.keys())[:k_features]\n",
    "    \n",
    "    def _compute_feature_utility(self, x, y, t, c, method = 'chi_square'):\n",
    "        n_docs_on_topic_present = 0\n",
    "        n_docs_on_topic_absent = 0\n",
    "        n_docs_off_topic_present = 0\n",
    "        n_docs_off_topic_absent = 0\n",
    "        for index, doc_tokens in enumerate(x):\n",
    "            if y[index] == c:\n",
    "                if t in doc_tokens:\n",
    "                    n_docs_on_topic_present += 1\n",
    "                else:\n",
    "                    n_docs_on_topic_absent += 1\n",
    "            else:\n",
    "                if t in doc_tokens:\n",
    "                    n_docs_off_topic_present += 1\n",
    "                else:\n",
    "                    n_docs_off_topic_absent += 1\n",
    "\n",
    "        n_docs_in_data = len(y)\n",
    "        n_docs_on_topic = n_docs_on_topic_present + n_docs_on_topic_absent\n",
    "        n_docs_off_topic = n_docs_off_topic_present + n_docs_off_topic_absent\n",
    "        n_docs_present = n_docs_on_topic_present + n_docs_off_topic_present\n",
    "        n_docs_absent = n_docs_on_topic_absent + n_docs_off_topic_absent\n",
    "        \n",
    "        e_on_topic_present = n_docs_present * n_docs_on_topic / n_docs_in_data\n",
    "        e_on_topic_absent = n_docs_absent * n_docs_on_topic / n_docs_in_data\n",
    "        e_off_topic_present = n_docs_present * n_docs_off_topic / n_docs_in_data\n",
    "        e_off_topic_absent = n_docs_absent * n_docs_off_topic / n_docs_in_data\n",
    "        \n",
    "        if method == 'chi_square':\n",
    "            chi = 0\n",
    "            chi += (n_docs_on_topic_present - e_on_topic_present)**2 / e_on_topic_present\n",
    "            chi += (n_docs_on_topic_absent - e_on_topic_absent)**2 / e_on_topic_absent\n",
    "            chi += (n_docs_off_topic_present - e_off_topic_present)**2 / e_off_topic_present\n",
    "            chi += (n_docs_off_topic_absent - e_off_topic_absent)**2 / e_off_topic_absent\n",
    "            return chi\n",
    "\n",
    "        if method == 'likelihood':\n",
    "            p_t = (n_docs_on_topic_present + n_docs_off_topic_present) / n_docs_in_data\n",
    "            p_1 = n_docs_on_topic_present / (n_docs_on_topic_present + n_docs_on_topic_absent)\n",
    "            p_2 = n_docs_off_topic_present / (n_docs_off_topic_present + n_docs_off_topic_absent)\n",
    "\n",
    "            lmbd = (p_t** n_docs_on_topic_present * (1 - p_t)** n_docs_on_topic_absent * p_t** n_docs_off_topic_present * (1 - p_t)** n_docs_off_topic_absent) / \\\n",
    "                (p_1** n_docs_on_topic_present * (1 - p_1)** n_docs_on_topic_absent * p_2** n_docs_off_topic_present * (1 - p_2)** n_docs_off_topic_absent)\n",
    "\n",
    "            return -2 * math.log(lmbd)\n",
    "\n",
    "        if method == 'MI':\n",
    "            return n_docs_on_topic_present / n_docs_in_data * math.log(n_docs_in_data * n_docs_on_topic_present / n_docs_on_topic * n_docs_present or 1, 2) + \\\n",
    "                n_docs_off_topic_present / n_docs_in_data * math.log(n_docs_in_data * n_docs_off_topic_present / n_docs_off_topic * n_docs_present or 1, 2) + \\\n",
    "                n_docs_on_topic_absent / n_docs_in_data * math.log(n_docs_in_data * n_docs_on_topic_absent / n_docs_on_topic * n_docs_absent or 1, 2) + \\\n",
    "                n_docs_off_topic_absent / n_docs_in_data * math.log(n_docs_in_data * n_docs_off_topic_absent / n_docs_off_topic * n_docs_absent or 1, 2)\n",
    "    \n",
    "    def _extract_vocabulary(self, docs):\n",
    "        token_list = []\n",
    "        for doc in docs:\n",
    "            token_list.extend(doc)\n",
    "        return np.unique(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training class 1\n",
      "Training class 2\n",
      "Training class 3\n",
      "Training class 4\n",
      "Training class 5\n",
      "Training class 6\n",
      "Training class 7\n",
      "Training class 8\n",
      "Training class 9\n",
      "Training class 10\n",
      "Training class 11\n",
      "Training class 12\n",
      "Training class 13\n",
      "Model saved!\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "clf = MultipleNBClassifier()\n",
    "clf.fit(x_train, y_train, k_features = 500, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training class 1\n",
      "Training class 2\n",
      "Training class 3\n",
      "Training class 4\n",
      "Training class 5\n",
      "Training class 6\n",
      "Training class 7\n",
      "Training class 8\n",
      "Training class 9\n",
      "Training class 10\n",
      "Training class 11\n",
      "Training class 12\n",
      "Training class 13\n",
      "Model saved!\n",
      "Training done\n",
      "Accuracy:  0.6102564102564103\n"
     ]
    }
   ],
   "source": [
    "METHOD = 'MI'\n",
    "clf = MultipleNBClassifier()\n",
    "# clf.read_model()\n",
    "clf.fit(x_train, y_train, k_features = 500, save = True, method = METHOD)\n",
    "\n",
    "acc = 0\n",
    "for index, doc in enumerate(x_train):\n",
    "    y_pred = clf.predict(doc)\n",
    "    if y_pred == y_train[index]:\n",
    "        acc += 1\n",
    "\n",
    "print('Accuracy: ', acc / len(y_train))\n",
    "\n",
    "with open('./hw3_sam (1).csv', newline='') as csvfile:\n",
    "    fieldnames = ['Id', 'Value']\n",
    "    optfile = open(\"./hw3_sam-{}.csv\".format(METHOD), 'w', newline='')\n",
    "    writer = csv.DictWriter(optfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    for row in rows:\n",
    "        y_pred = clf.predict(document_token[ int(row['Id']) ])\n",
    "        writer.writerow({'Id': int(row['Id']), 'Value': y_pred})\n",
    "\n",
    "    optfile.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw3_sam (1).csv', newline='') as csvfile:\n",
    "    fieldnames = ['Id', 'Value']\n",
    "    optfile = open('./hw3_sam.csv', 'w', newline='')\n",
    "    writer = csv.DictWriter(optfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    for row in rows:\n",
    "        y_pred = clf.predict(document_token[ int(row['Id']) ])\n",
    "        writer.writerow({'Id': int(row['Id']), 'Value': y_pred})\n",
    "\n",
    "    optfile.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}