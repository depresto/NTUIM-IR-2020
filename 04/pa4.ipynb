{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [8, 13, 20]\n",
    "DATA_DIR = './IRTM'\n",
    "OPT_DIR = './OPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(content):\n",
    "    punct = str.maketrans('', '', string.punctuation.replace(\"-\", \"\"))\n",
    "    # Tokenize content\n",
    "    tokens = content.translate(punct).replace('\\n', '').split(' ')\n",
    "    # Lower case\n",
    "    lower_tokens = list(map(lambda word: word.lower(), tokens))\n",
    "    # Stemming using Porter's algorithm\n",
    "    porter = PorterStemmer()\n",
    "    stemed_tokens = list(map(lambda word: porter.stem(word), lower_tokens))\n",
    "    # Stopword removal\n",
    "    filtered_tokens = [\n",
    "        word for word in stemed_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_tokens\n",
    "\n",
    "def read_dataset():\n",
    "    filename_list = []\n",
    "    full_filenames = os.listdir('./IRTM')\n",
    "    for full_filename in full_filenames:\n",
    "        filename, extname = full_filename.split('.')\n",
    "        if extname == 'txt':\n",
    "            filename_list.append(int(filename))\n",
    "\n",
    "    document_token = {}\n",
    "    for filename in filename_list:\n",
    "        file = open('{}/{}.txt'.format('./IRTM', filename), 'r')\n",
    "        content = file.read()\n",
    "        file.close()\n",
    "\n",
    "        tokens = tokenize_data(content)\n",
    "\n",
    "        filtered_tokens = filter(lambda x: x != '', tokens)\n",
    "        filtered_tokens = filter(lambda x: x[0] not in string.punctuation, filtered_tokens)\n",
    "        filtered_tokens = list(filter(lambda x: not str.isdigit(x[0][0]), filtered_tokens))\n",
    "\n",
    "        document_token[filename] = filtered_tokens\n",
    "\n",
    "    return document_token\n",
    "\n",
    "def cosine_similarity(d1, d2):\n",
    "    return d1 @ d2 / (((d1 @ d1) ** 0.5) * ((d2 @ d2) ** 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_token = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_token_count = {}\n",
    "for filename, tokens in document_token.items():\n",
    "    document_token_count[filename] = Counter(tokens)\n",
    "\n",
    "document_token_frequency = {}\n",
    "for filename, token_counter in document_token_count.items():\n",
    "    token_length = sum(token_counter.values())\n",
    "    frequency = {}\n",
    "    for token, count in token_counter.items():\n",
    "        frequency[token] = count / token_length\n",
    "    document_token_frequency[filename] = frequency\n",
    "\n",
    "global_token = []\n",
    "for token_counter in document_token_count.values():\n",
    "    global_token.extend(token_counter.keys())\n",
    "global_token_occurence = Counter(global_token)\n",
    "\n",
    "inverse_document_frequency = {}\n",
    "for filename, token_counter in document_token_count.items():\n",
    "    inverse_frequency = {}\n",
    "    for token in token_counter.keys():\n",
    "        token_occurence = global_token_occurence[token]\n",
    "        inverse_frequency[token] = math.log10(len(document_token_count) / token_occurence)\n",
    "    inverse_document_frequency[filename] = inverse_frequency\n",
    "\n",
    "document_tf_idf = {}\n",
    "for document, token_frequency in document_token_frequency.items():\n",
    "    tf_idf = {}\n",
    "    for token, frequency in token_frequency.items():\n",
    "        tf_idf[token] = frequency * inverse_document_frequency[document][token]\n",
    "    document_tf_idf[document] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = sorted(global_token_occurence.items(), key=lambda item: item[0])\n",
    "dictionary_token = list(map(lambda item: item[0], dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tf_idf_list = []\n",
    "for tf_idfs in document_tf_idf.values():\n",
    "    tf_idf_list = [0] * len(dictionary_token)\n",
    "    for token, tf_idf in tf_idfs.items():\n",
    "        tf_idf_list[dictionary_token.index(token)] = tf_idf\n",
    "    document_tf_idf_list.append(tf_idf_list)\n",
    "\n",
    "document_tf_idf_arr = np.array(document_tf_idf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_hac(documents):\n",
    "    N = len(documents)\n",
    "    C = {}\n",
    "    I = [False] * N\n",
    "    pbar_progress = tqdm(range(N), desc=\"Load similarity\", position=0, leave=True)\n",
    "    for n in pbar_progress:\n",
    "        for i in range(N):\n",
    "            C[n, i] = cosine_similarity(documents[n], documents[i])\n",
    "        I[n] = True\n",
    "    A = []\n",
    "\n",
    "    pbar_merge = tqdm(range(N - 1), desc=\"Merge clusters\", position=0, leave=True)\n",
    "    for k in pbar_merge:\n",
    "        max_sum = {\n",
    "            'i': None,\n",
    "            'm': None,\n",
    "            'cosine_similarity': 0\n",
    "        }\n",
    "        for i in range(N):\n",
    "            if I[i] is False:\n",
    "                continue\n",
    "            for m in range(N):\n",
    "                if I[m] is False or i == m:\n",
    "                    continue\n",
    "                if C[i, m] > max_sum['cosine_similarity']:\n",
    "                    max_sum['cosine_similarity'] = C[i, m]\n",
    "                    max_sum['i'] = i\n",
    "                    max_sum['m'] = m\n",
    "\n",
    "        i = max_sum['i']\n",
    "        m = max_sum['m']\n",
    "        sim = cosine_similarity(documents[i], documents[m])\n",
    "        pbar_merge.set_description(\"Similarity: {}. Merge {}, {}\".format(sim, i, m))\n",
    "        A.append({ 'i': i, 'm': m, 'cosine_similarity': sim })\n",
    "\n",
    "        for j in range(N):\n",
    "            # Complete link\n",
    "            C[i, j] = min(cosine_similarity(documents[i], documents[j]), cosine_similarity(documents[m], documents[i]))\n",
    "            C[j, i] = min(cosine_similarity(documents[j], documents[i]), cosine_similarity(documents[m], documents[j]))\n",
    "        I[m] = False\n",
    "\n",
    "    return C, A\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Load similarity: 100%|██████████| 1095/1095 [00:17<00:00, 64.17it/s]\n",
      "Similarity: 0.004766354814710763. Merge 13, 153: 100%|██████████| 1094/1094 [04:23<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "C, A = simple_hac(document_tf_idf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = []\n",
    "for k in k_list:\n",
    "    clusters = []\n",
    "    for i in range(len(document_tf_idf_arr)):\n",
    "        clusters.append([i + 1])\n",
    "    clusters_len = len(clusters)\n",
    "\n",
    "    for a in A:\n",
    "        clusters[a['i']].extend(clusters[a['m']])\n",
    "        clusters[a['m']] = None\n",
    "        clusters_len -= 1\n",
    "\n",
    "        if clusters_len == k:\n",
    "            clusters = list(filter(lambda cluster: cluster is not None, clusters))\n",
    "            k_clusters.append(clusters)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, k in enumerate(k_list):\n",
    "    with open('{}/{}.txt'.format(OPT_DIR, k), 'w') as opt_file:\n",
    "        for doc_ids in k_clusters[index]:\n",
    "            for doc_id in sorted(doc_ids):\n",
    "                opt_file.write(\"{}\\n\".format(doc_id))\n",
    "            opt_file.write(\"\\n\")"
   ]
  }
 ]
}